{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose,Resize,ToTensor,Normalize,RandomHorizontalFlip,RandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "patch_size = 16\n",
    "latent_size = 768\n",
    "n_channels = 3\n",
    "num_heads = 12\n",
    "num_encoders = 12\n",
    "dropout = 0.1\n",
    "num_classes = 10\n",
    "size = 224\n",
    "\n",
    "epochs = 10\n",
    "base_lr = 10e-3\n",
    "weight_decay = 0.03\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 patch_size = patch_size,\n",
    "                 latent_size = latent_size,\n",
    "                 n_channels = n_channels,\n",
    "                 batch_size = batch_size,\n",
    "                 device = device):\n",
    "        super(InputEmbedding,self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.latent_size = latent_size\n",
    "        self.n_channels = n_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        self.input_size = self.patch_size * self.patch_size * self.n_channels\n",
    "\n",
    "        # Linear Projection Layer\n",
    "        self.LinearProjection = nn.Linear(self.input_size,\n",
    "                                       self.latent_size)\n",
    "        \n",
    "        # Class Token\n",
    "        self.ClassToken = nn.Parameter(torch.randn(self.batch_size,\n",
    "                                                   1,\n",
    "                                                   self.latent_size))  # Creates a class token vector of size (4,1,768)\n",
    "        \n",
    "        # Positional Embedding\n",
    "        self.PositionalEmbedding = nn.Parameter(torch.randn(self.batch_size,\n",
    "                                                            1,\n",
    "                                                            self.latent_size)) # Creates a positional embedding of size (4,1,768)\n",
    "        \n",
    "    def forward(self,\n",
    "                input_data):\n",
    "        \n",
    "        # Patchify the input data --> Convert the image of size (224,224,3) to (196,768) [ where 196 = (224*224)/(16*16) Num_patches, 768 = (16*16*3) Flattened vector of every patch \n",
    "        patches = einops.rearrange(\n",
    "            input_data, 'b c (h h1) (w w1) -> b (h w) (h1 w1 c)', h1=self.patch_size, w1=self.patch_size\n",
    "            ) \n",
    "        \n",
    "        linear_projection = self.LinearProjection(patches)  # Convert the (4,196,768) to (4,196,768) :: the first 768 is just by chance and depend on patch size later one is latent_dim\n",
    "        b,n,p = linear_projection.shape\n",
    "\n",
    "        linear_projection = torch.cat([self.ClassToken,linear_projection],dim=1) # Concatentate classtoken to linear project (4,197,768)\n",
    "        \n",
    "        positional_embedding = einops.repeat(self.PositionalEmbedding, 'b 1 d -> b m d', m=n+1) # Converted pos_embed (4,1,768) --> (4,197,768) \n",
    "\n",
    "        linear_projection += positional_embedding # Added pos_embed to linear_proj (4,196,768)\n",
    "\n",
    "        return linear_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.randn(batch_size,3,224,224).to(device)\n",
    "test_class = InputEmbedding().to(device)\n",
    "embed_patches = test_class(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 latent_size = latent_size,\n",
    "                 num_heads = num_heads,\n",
    "                 dropout = dropout,\n",
    "                 device = device):\n",
    "           super(EncoderBlock,self).__init__()\n",
    "\n",
    "           self.latent_size = latent_size\n",
    "           self.num_heads = num_heads\n",
    "           self.dropout = dropout\n",
    "           self.device = device\n",
    "\n",
    "           # Normalization Layer\n",
    "           self.Normalization = nn.LayerNorm(self.latent_size)\n",
    "\n",
    "           # Multi-Headed Attention\n",
    "           self.MultiHead = nn.MultiheadAttention(\n",
    "                 embed_dim = self.latent_size,\n",
    "                 num_heads = self.num_heads,\n",
    "                 dropout = self.dropout\n",
    "           )\n",
    "\n",
    "           # MLP Layer\n",
    "           self.encoder_MLP = nn.Sequential(\n",
    "                 nn.Linear(self.latent_size,self.latent_size*4),\n",
    "                 nn.GELU(),\n",
    "                 nn.Dropout(self.dropout),\n",
    "                 nn.Linear(self.latent_size*4,self.latent_size),\n",
    "                 nn.Dropout(self.dropout)\n",
    "           )\n",
    "\n",
    "    def forward(self,embed_patches):\n",
    "          \n",
    "          # First Normalization Layer output\n",
    "          first_normout = self.Normalization(embed_patches)\n",
    "\n",
    "          # Output from the MultiHeadedAttention  \n",
    "          attention_out = self.MultiHead(first_normout,first_normout,first_normout)[0]\n",
    "\n",
    "          # First Residual Connection\n",
    "          residual_out = attention_out + embed_patches\n",
    "\n",
    "          # Second Norm output  \n",
    "          second_normout = self.Normalization(residual_out)\n",
    "\n",
    "          # Output from MLP\n",
    "          mlp_out = self.encoder_MLP(second_normout)\n",
    "          \n",
    "          # Second residual connection\n",
    "          output = mlp_out + residual_out\n",
    "          \n",
    "          return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = EncoderBlock()\n",
    "output = enc(embed_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_encoders = num_encoders,\n",
    "                 latent_size = latent_size,\n",
    "                 num_classes = num_classes,\n",
    "                 dropout = dropout,\n",
    "                 device = device,\n",
    "                 ):\n",
    "        super(ViT,self).__init__()\n",
    "\n",
    "        self.num_encoders = num_encoders\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Embedding Layer\n",
    "        self.embedding = InputEmbedding()\n",
    "\n",
    "        # Encoder Stack\n",
    "        self.enc_stack = nn.ModuleList([EncoderBlock() for i in range(self.num_encoders)])\n",
    "\n",
    "        # VIT-MLP\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.LayerNorm(self.latent_size),\n",
    "            nn.Linear(self.latent_size,self.latent_size),\n",
    "            nn.Linear(self.latent_size,self.num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, test_input):\n",
    "\n",
    "        # Finding Patch Embeddings\n",
    "        encoder_output = self.embedding(test_input)\n",
    "\n",
    "        # Looping through the encoder stack\n",
    "        for encoder_layer in self.enc_stack:\n",
    "            encoder_output = encoder_layer(encoder_output)\n",
    "\n",
    "        # Extracting the class token embedding\n",
    "        class_token_embedding = encoder_output[:,0]\n",
    "\n",
    "        # Sending it through the MLP\n",
    "        output = self.MLP(class_token_embedding)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor([[ 0.0715,  0.5755, -0.0244,  0.1811,  0.4734, -0.2432, -0.4321,  0.3122,\n",
      "         -0.2350, -0.0411],\n",
      "        [-0.1588, -0.1795, -0.0497,  0.2420, -0.6304, -0.0514, -0.8325, -0.0392,\n",
      "         -0.3193,  0.4923],\n",
      "        [-0.0328,  0.1791, -0.4711,  0.2433, -0.0438, -0.4839, -0.3521,  0.0707,\n",
      "          0.1584, -0.0077],\n",
      "        [-0.2587,  0.0469,  0.4143,  0.5155, -0.3633, -0.3819, -0.6325,  0.0724,\n",
      "          0.0907,  0.6130]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = ViT()\n",
    "\n",
    "test_input = torch.randn(4,3,224,224)\n",
    "output = model(test_input)\n",
    "\n",
    "print(output.shape)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 1196554\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Total Trainable Parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT(\n",
       "  (embedding): InputEmbedding(\n",
       "    (LinearProjection): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (MLP): Sequential(\n",
       "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (2): Linear(in_features=768, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
